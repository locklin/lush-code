;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; LUSH Lisp Universal Shell
;;;   Copyright (C) 2002 Leon Bottou, Yann Le Cun, AT&T Corp, NECI.
;;; Includes parts of TL3:
;;;   Copyright (C) 1987-1999 Leon Bottou and Neuristique.
;;; Includes selected parts of SN3.2:
;;;   Copyright (C) 1991-2001 AT&T Corp.
;;;
;;; This program is free software; you can redistribute it and/or modify
;;; it under the terms of the GNU General Public License as published by
;;; the Free Software Foundation; either version 2 of the License, or
;;; (at your option) any later version.
;;;
;;; This program is distributed in the hope that it will be useful,
;;; but WITHOUT ANY WARRANTY; without even the implied warranty of
;;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
;;; GNU General Public License for more details.
;;;
;;; You should have received a copy of the GNU General Public License
;;; along with this program; if not, write to the Free Software
;;; Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111, USA
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;; $Id: gb-trainers.lsh,v 1.2 2002-11-26 20:22:59 profshadoko Exp $
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(libload "gb-modules")
(libload "gb-meters")


#? **** Learning Algorithms
;; {<author> Yann LeCun}
;; Various learning algorithm classes are defined
;; to train learning machines. Learning machines are
;; generally subclasses of <gb-module>. Learning algorithm
;; classes include gradient descent for supervised 
;; learning, and others.

#? ** supervised-gradient
;; A simple learning algorithm for supervised stochastic gradient
;; training of a classifier with discrete class labels. The machine's 
;; fprop method must have four arguments: input, output, energy, 
;; and desired output. A call to the machine's fprop must
;; look like this:
;; {<code>
;;    (==> machine fprop input output desired energy)
;; </code>}
;; where <output> must be a <class-state>, <desired> an
;; idx0 of int (integer scalar), and <energy> and <idx0-ddstate>.
;; The meter passed to the training and testing methods
;; should be a <classifier-meter>, or any meter whose
;; update method looks like this:
;; {<code>
;;    (==> meter update output desired energy)
;; </code>}
;; where <output> must be a <class-state>, <desired> an
;; idx0 of int, and <energy> and <idx0-ddstate>.
;; The trainable parameter object must understand the following 
;; methods:
;; {<ul>
;;  {<li> {<c> (==> param set-eta eta)}: set the learning rate.}
;;  {<li> {<c> (==> param clear-dx)}: clear the gradients.}
;;  {<li> {<c> (==> param update)}: update the parameters.}
;; }
;; If the diagonal hessian estimation is to be used, the param
;; object must also understand:
;; {<ul>
;;  {<li> {<c> (==> param set-gamma gamma)}: set the learning rate for diag hessian terms.}
;;  {<li> {<c> (==> param set-mu mu)}: set blowup prevention fudge factor.}
;;  {<li> {<c> (==> param clear-ddx)}: clear the second derivatives.}
;;  {<li> {<c> (==> param update-bbprop)}: update second derivatives.}
;;  {<li> {<c> (==> param compute-epsilons)}: set the per-parameter learning rates to 
;;   inverse second derivatives.}
;; }
(defclass supervised-gradient object
  machine
  param
  input
  output
  desired
  energy
  age
  )

#? (new supervised-gradient <m> <p> [<e> <in> <out> <des>])
;; create a new <supervised-gradient> trainer. Arguments are as follow:
;; {<ul>
;;  {<li> <m>: machine to be trained.}
;;  {<li> <p>: trainable parameter object of the machine.}
;;  {<li> <e>: energy object (by default an idx0-ddstate).}
;;  {<li> <in>: input object (by default an idx3-ddstate).}
;;  {<li> <out>: output object (by default a class-state).}
;;  {<li> <des>: desired output (by default an idx0 of int).}
;; }
(defmethod supervised-gradient supervised-gradient (m p &optional e in out des)
  (setq machine m)
  (setq param p)
  (setq age 0)
  (if e
      (setq energy e)
    ;; set energy slot to default idx0-ddstate
    (setq energy (new idx0-ddstate))
    (:energy:dx 1.0)
    (:energy:ddx 0.0))
  (if in
      (setq input in)
    (setq input (new idx3-ddstate 1 1 1)))
  (if out
      (setq output out)
    (setq output (new class-state 2)))
  (if des
      (setq desired des)
    (setq desired (int-matrix)))
  ())


#? (==> supervised-gradient train <dsource> <n> <eta> <mtr>)
;; train with stochastic gradient on data source <dsource> for 
;; <n> iterations with global learning rate <eta>. Record performance 
;; in <mtr>. This simply trains on the next <n> examples coming out
;; of the data source.
;; <mtr> must understand the following methods:
;; {<code>
;;   (==> mtr update age output desired energy)
;;   (==> mtr info)
;; </code>}
;; where <age> is the number of calls to parameter updates so far,
;; <output> is the machine's output (most likely a <class-state>),
;; <desired> is the desired output (most likely an idx0 of int),
;; and <energy> is an <idx0-state>.
;; The <info> should return a list of relevant measurements.
(defmethod supervised-gradient train (ds n eta mtr)
  (==> param set-eta eta)
  (repeat n
    (==> ds fprop input desired)
    (==> machine fprop input output desired energy)
    (==> mtr update age output desired energy)
    (==> param clear-dx)
    (==> machine bprop input output desired energy)
    (==> param update)
    (incr age)
    ;; (print (==> ds tell) (desired) :machine:mout:x)
    (==> ds next))
  (==> mtr info))

#? (==> supervised-gradient train-dsource <dsource> <eta> <mtr>)
;; performs a stochastic gradient training epoch over all the samples 
;; in data source <dsource>.
;; <eta> is a global learning rate, and <mtr> an appropriate meter.
(defmethod supervised-gradient train-dsource (ds eta mtr)
  (==> ds seek 0)
  (==> mtr clear)
  (==> this train ds (==> ds size) eta mtr))

#? (==> supervised-gradient test-dsource <dsource> <mtr>)
;; measures the performance over all the samples of data source <dsource>.
;;<mtr> must be an appropriate meter.
(defmethod supervised-gradient test-dsource (ds mtr)
  (==> ds seek 0)
  (==> mtr clear)
  (repeat (==> ds size)
    (==> ds fprop input desired)
    (==> machine fprop input output desired energy)
    (==> mtr update age output desired energy)
    (==> ds next)))

#? (==> supervised-gradient test-pattern <dsource> <i> <mtr>)
;; measures the performance over a single sample of data source <dsource>.
;; This leaves the internal state of the meter unchanged, and
;; can be used for a quick test of a whether a particular pattern
;; is correctly recognized or not.
(defmethod supervised-gradient test-pattern (ds i mtr)
  (==> ds seek i)
  (==> ds fprop input desired)
  (==> machine fprop input output desired energy)
  (==> mtr test output desired energy))

#? (==> supervised-gradient compute-diaghessian <dsource> <n> <gamma> <mu>)
;; Compute per-parameter learning rates (epsilons) using the
;; stochastic diaginal levenberg marquardt method (as described in
;; LeCun et al.  "efficient backprop", available at 
;; {<hlink> http://yann.lecun.com}).  This method computes positive 
;; estimates the second derivative of the objective function with respect 
;; to each parameter using the Gauss-Newton approximation.  <dsource> is
;; a data source, <n> is the number of patterns (starting at the current
;; point in the data source) on which the
;; estimate is to be performed. <gamma> is the "learning rate" used to
;; update running average estimate of the second derivatives, and <mu>
;; is the blowup prevention fudge factor.  Each parameter-specific 
;; learning rate epsilon_i is computed as 1/(H_ii + mu), where H_ii
;; are the diagonal Gauss-Newton estimates.
(defmethod supervised-gradient compute-diaghessian (ds n gamma mu)
  (==> param set-gamma gamma)
  (==> param set-mu mu)
  (repeat n
    (==> ds fprop input desired)
    (==> machine fprop input output desired energy)
    (==> param clear-dx)
    (==> machine bprop input output desired energy)
    (==> param clear-ddx)
    (==> machine bbprop input output desired energy)
    (==> param update-bbprop)
    (==> ds next))
  (==> param compute-epsilons)
  (list ((idx-inf :param:epsilons)) ((idx-sup :param:epsilons))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
