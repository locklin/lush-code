;;;   Simple Linear Associator
;;;   Reference: PDP vol 1, pp 62-63
;;;
;;;   (C) Copyright Neuristique, 1989


;;; === Build a NxM linear associator

(de SLA-build-net(n m)
    (nlf-lin 1 1)              ;linear transfert function
    (build-net
     '((input n) (output m))				;two layers
     '((input output)) ) )      ;fully connected 


;;; === Retrieval functions

; The standard library function (test-pattern n)
; needs no modification.


;;; === Learning function: Hebbian rule

(de SLA-Hebb-iteration (patt-number)
    
    ;retrieval
    (present-pattern input-layer patt-number)
    (update-state output)
    
    ;tests and display
    (present-desired desired-layer patt-number)
    (setq good-answer 
    	  (classify patt-number))
    (setq local-error 
    	  (2/ (mean-sqr-dist (state output) (state desired-layer))))
    (disp-basic-iteration patt-number) 
    
    ;learning
    (copy-nfield output-layer n-grad desired-layer n-val)
    (incr age)
    (update-weight) )


(de SLA-Hebb-learn(it)
    (repeat it 
	    (SLA-Hebb-iteration current-pattern)
	    (setq current-pattern 
            	  (next-pattern current-pattern)) ) )


;;; === Learning function: Delta rule, 
;;;         also called Widrow-Hoff rule.
;;; (note)  This is a special case of GBP!

(de SLA-Delta-iteration (patt-number)
    
    ;retrieval
    (present-pattern input-layer patt-number)
    (update-state output)
    
    ;tests and display
    (present-desired desired-layer patt-number)
    (setq good-answer 
    	  (classify patt-number))
    (setq local-error 
    	  (2/ (mean-sqr-dist (state output) (state desired-layer))))
    (disp-basic-iteration patt-number) 
    
    ;learning
    (init-grad output desired-layer)
    (incr age)
    (update-weight) )


(de SLA-Delta-learn(it)
    (repeat it 
	    (SLA-Delta-iteration current-pattern)
	    (setq current-pattern 
            	  (next-pattern current-pattern)) ) )
