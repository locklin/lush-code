;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; LUSH Lisp Universal Shell
;;;   Copyright (C) 2002 Leon Bottou, Yann Le Cun, AT&T Corp, NECI.
;;; Includes parts of TL3:
;;;   Copyright (C) 1987-1999 Leon Bottou and Neuristique.
;;; Includes selected parts of SN3.2:
;;;   Copyright (C) 1991-2001 AT&T Corp.
;;;
;;; This program is free software; you can redistribute it and/or modify
;;; it under the terms of the GNU General Public License as published by
;;; the Free Software Foundation; either version 2 of the License, or
;;; (at your option) any later version.
;;;
;;; This program is distributed in the hope that it will be useful,
;;; but WITHOUT ANY WARRANTY; without even the implied warranty of
;;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
;;; GNU General Public License for more details.
;;;
;;; You should have received a copy of the GNU General Public License
;;; along with this program; if not, write to the Free Software
;;; Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111, USA
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;; $Id: xor-example.lsh,v 1.1 2002-08-19 07:04:08 profshadoko Exp $
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; a small example of how you might setup a 2-layer net to
;; learn the infamous exclusive OR.

#? Learning the proverbial XOR with backprop
;; the content of this file is a complete example
;; of how to build a single hidden layer backprop
;; net to solve the XOR problem. Yeah, it's a lame
;; example. Just have a look at the source code.

;; load fully conneted net library
(mload "net-ff")
;; load trainer class
(mload "trainer")
;; load workbench class
(mload "workbench")
;; load  db class
(mload "libdb/base-db")

;; create a "param" the will be inserted in the trainer
;; an idx1-ddparam uses the diagonal Levenberg-Marquardt algorithm
;; (diagonal hessian estimation, then regular stochastic gradient descent).
;; the parameters are 
;; REAL parameters will be allocated from within this param by passing to the
;; constructor of the various adaptive modules
;; 0    : the initial number of parameters in the param 
;;        (this should probably be always 0)
;; 0.1  : the global learning rate
;; 0.02 : the running average constant for the diag hessian calculation
;; 0.02 : the term added to the diag hessian in the denominator of the individual
;;        learning rates (to prevent them from blowing up if 2nd deriv is 0).
;; 1000 : the size of the param to preallocate. Making this number large enough
;;        to old all the parameters prevents memory fragmentation due to 
;;        reallocation when real parameters are allocated within the param.
(setq theparam (new idx1-ddparam 0 0.1 0.02 0.02 1000))

;; make a one hidden layer fully-connected net.
;; this net is replicable (a la SDNN), therefore its inputs and outputs
;; are 3-dimensional (idx3-ddstate), one dimension for the neuron, and
;; two spatial dimensions. In this example the network is not replicated
;; so the last two dimensions are 1.
;; arguments:
;; 2 2 2: sizes of input, hidden layer, and output
;; 1 1 : initial size of spatial replication. net-ff can be turned into SDNN
;;       with spatial replication, this says how much memory to preallocate
;;       for the states (the actual size is determined by the size of the input,
;;       and is automatically determined). These numbers are only necessary to 
;;       prevent memory fragmentation.
;; theparam: the param in which all the free parameters of the network 
;;       will be allocated. bprop messages sent to the network compute
;;       the gradients in the param. Update messages setn to the param
;;       compute the new weights for the network.
(setq thenet (new net-ff 2 2 2 1 1 theparam))

;; now build the trainer. The trainer contains the network, the param,
;; a preprocessor (here, the preprocessor module does nothing)
;; a postprocessor (a classifier), and a cost module.
;; It also contains intermediate states that store intermediate
;; states between the various modules (raw-input, machine-input,
;; machine-output, postproc-output, desired, energy).
(let ((clss (int-matrix 2)))
  (clss () '(0 1))
  (setq thetrainer 
	(new trainer
	     (new id-module)		; preprocessor
	     thenet			; the trainable machine
	     theparam			; its adaptive parameters
	     (new max-classer clss)	; the classifier
	     (new edist-cost clss 1 1 [ [-1 1] [1 -1] ]) ; the cost module
					; with prototypes. The matrix contains
					; the output target vector for each class.
	     (lambda (from to)		; the function that transforms patterns
	       (==> to resize 2 1 1)	; from the database into things the
					; network understands
	       (copy-matrix from (redim :to:x 2))) ;  transforms the desired answer
	     (lambda (from to) (to (from 0))) ; from the db into something the cost
					; module understands
	     (new idx3-state 2 1 1)	; raw-input
	     (new idx3-ddstate 2 1 1)	; machine-input
	     (new idx3-ddstate 2 1 1)	; machine-output
	     (new class-state 2)	; postproc-output
	     (int-matrix))))		; desired-output

; a XOR with two outputs
(setq traindb 
      (new idx-db
           [ [[[-1 -1]]] [[[-1  1]]] [[[ 1  1]]] [[[ 1 -1]]] ]
	   [ [0] [1] [0] [1] ]))

(setq testdb 
      (new idx-db
           [ [[[-1 -1]]] [[[-1  1]]] [[[ 1  1]]] [[[ 1 -1]]] ]
	   [ [0] [1] [0] [1] ]))

;; a classifier-meter measures classification errors
(setq trainmeter (new classifier-meter 4))
(setq testmeter  (new classifier-meter 4))

;; make a new workbench containing the trainer the databases and the meters
(setq wk (new workbench 
	      "junk.log"		; a log file
	      thetrainer		; the trainer
	      traindb			; training database
	      testdb			; testing database
	      trainmeter		; 
	      testmeter))

;; initialize the network weights
;; this method does the right thing for fanin scaling
;; weights are picked randomly between a / fanin^(1/b)
;; where a and b are the arguments of forget.
(==> thenet forget 1 2)

;; estimate second derivative on 100 iterations, using gamma=0.02 and mu=0.02
;; and set individual espilons
(==> wk compute-diaghessian 100 0.02 0.02)

;; do 32 training iterations with learning rate eta=0.01
(==> wk train 32 0.005)
